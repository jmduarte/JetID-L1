{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JK0wr2onBn4d"
   },
   "source": [
    "# Quantization aware (QKeras) DeepSet Permutation Invariant NN for Jet tagging using jet constituents from HLS data implemented by Patrick ( from paper https://arxiv.org/abs/1703.06114 )\n",
    "\n",
    "##   Original code from: https://github.com/bb511/know_dist\n",
    "\n",
    "## Author: Andre Sznajder\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3006,
     "status": "ok",
     "timestamp": 1616956528247,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "WrNOdwasBsHc",
    "outputId": "403bd6fb-ede6-43c8-9cf7-235cccf77806"
   },
   "outputs": [],
   "source": [
    "#!fusermount -u drive\n",
    "#! pip install einops\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive', force_remount=True)\n",
    "#data_dir = '/content/gdrive/My Drive/Colab Notebooks/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3272,
     "status": "ok",
     "timestamp": 1616956528520,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "MhGGIDIvBu2V",
    "outputId": "20e67e9a-104e-4ce2-ac75-540ae8e1f459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.8.0\n",
      "Number of available GPUs : 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(f\"TensorFlow {tf.__version__}\")\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    print(f\"Number of available GPUs : {len(gpus)}\")\n",
    "    tf.config.set_visible_devices(gpus[0],\"GPU\")\n",
    "    tf.config.experimental.set_memory_growth(gpus[0],True)\n",
    "else:\n",
    "    print(\"No GPU available, using CPU !!!\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the train and test data as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3269,
     "status": "ok",
     "timestamp": 1616956528521,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "TcXokNduBn4g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded X_train_val ----> shape: (589600, 32, 3)\n",
      "Loaded X_test      ----> shape: (290400, 32, 3)\n",
      "Loaded Y_train_val ----> shape: (589600, 32, 3)\n",
      "Loaded Y_test      ----> shape: (290400, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "#Data PATH\n",
    "DATA_PATH = '/Users/sznajder/WorkM1/workdir/data/'\n",
    "\n",
    "nconstit = 32\n",
    "\n",
    "X_train_val = np.load(\"../../data/X_train_val_nconst_{}.npy\".format(nconstit))\n",
    "X_test = np.load(\"../../data/X_test_nconst_{}.npy\".format(nconstit))\n",
    "Y_train_val = np.load(\"../../data/Y_train_val_nconst_{}.npy\".format(nconstit))\n",
    "Y_test = np.load(\"../../data/Y_test_nconst_{}.npy\".format(nconstit))\n",
    "\n",
    "print(\"Loaded X_train_val ----> shape:\", X_train_val.shape)\n",
    "print(\"Loaded X_test      ----> shape:\", X_test.shape)\n",
    "print(\"Loaded Y_train_val ----> shape:\", X_train_val.shape)\n",
    "print(\"Loaded Y_test      ----> shape:\", X_test.shape)\n",
    "\n",
    "nfeat = X_train_val.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6350,
     "status": "ok",
     "timestamp": 1616956531619,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "aqEAX7zIBn40",
    "outputId": "f71a892c-d086-4df8-9e11-b1116e448764"
   },
   "source": [
    "## Define DeepSet Permutation Equivariant Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline keras model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Nadam\n",
    "from tensorflow.keras.layers import BatchNormalization, Input, Activation, Dense, Conv1D, Add, RepeatVector\n",
    "from tensorflow.keras.layers import Flatten, Reshape, GlobalAveragePooling1D, Concatenate, UpSampling1D, AveragePooling1D, MaxPooling1D  \n",
    "from tensorflow.keras import utils\n",
    "from qkeras import *\n",
    "\n",
    "import tensorflow.keras.layers as KL\n",
    "\n",
    "class PermutationEquivariantMax(KL.Layer):\n",
    "    \"\"\"Permutation equivariant neural network layer with max operation.\"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(PermutationEquivariantMax, self).__init__()\n",
    "        self.gamma = KL.Dense(dim)\n",
    "        self.lambd = KL.Dense(dim, use_bias=False)\n",
    "\n",
    "    def call(self, inputs: np.ndarray, **kwargs):\n",
    "        x_maximum = tf.reduce_max(inputs, axis=1, keepdims=True)\n",
    "        x_maximum = self.lambd(x_maximum)\n",
    "        x = self.gamma(inputs)\n",
    "        x = x - x_maximum\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PermutationEquivariantMean(KL.Layer):\n",
    "    \"\"\"Permutation equivariant neural network layer with mean operation.\"\"\"\n",
    "\n",
    "    def __init__(self, dim, kernel_quantizer, bias_quantizer):\n",
    "        super(PermutationEquivariantMean, self).__init__()\n",
    "        self.gamma = QDense(dim, kernel_quantizer, bias_quantizer, name='dense_1' )          # multiply features by weights and add bias, sharing the weights and bias. Returns #dim embeded features per element \n",
    "        self.lambd = QDense(dim, kernel_quantizer, name='densenobias_1',use_bias=False)      # multiply features by weights, sharing the weights over elements. Returns #dim embeded features per element\n",
    "        self.dim = dim\n",
    "        \n",
    "    def call(self, inputs: np.ndarray, **kwargs):\n",
    "        x_mean = GlobalAveragePooling1D(name='avgpool_1')(inputs) # returns a tensor of size (#elements,#features) containing the mean of each individual feature over elements \n",
    "        x_mean = Reshape((1,-1),name='reshap')(x_mean)      # reshape tensor to original 3D format (batch, 1, nfeat)\n",
    "#        x_mean = UpSampling1D(size=self.dim,name='upsampl_1')(x_mean)    # make #(nconstit) copies of tensor along axis=1\n",
    "\n",
    "        x_mean = self.lambd(x_mean)   # multiply each mean feature by a weights returning #dim embeded features per element (λI)\n",
    "        x = self.gamma(inputs)        # returns #dim embeded features per element \n",
    "        x = x - x_mean                # get deviation of X from mean features (λI) \n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"dim\": self.dim,\n",
    "                \"kernel_quantizer\": kernel_quantizer,\n",
    "                \"bias_quantizer\": bias_quantizer,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DeepSet Permutation Equivariant Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with max # of contituents =  32\n",
      "Number of node features =  3\n",
      "Quantization with nbits= 8\n",
      "Quantization of integer part= 0\n",
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 13:55:54.070662: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-06 13:55:54.070951: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inp (InputLayer)            [(None, 32, 3)]           0         \n",
      "                                                                 \n",
      " BatchNorm (BatchNormalizati  (None, 32, 3)            12        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " permutation_equivariant_mea  (None, 32, 32)           224       \n",
      " n (PermutationEquivariantMe                                     \n",
      " an)                                                             \n",
      "                                                                 \n",
      " qrelu_phi1 (QActivation)    (None, 32, 32)            0         \n",
      "                                                                 \n",
      " permutation_equivariant_mea  (None, 32, 32)           2080      \n",
      " n_1 (PermutationEquivariant                                     \n",
      " Mean)                                                           \n",
      "                                                                 \n",
      " qrelu_phi2 (QActivation)    (None, 32, 32)            0         \n",
      "                                                                 \n",
      " permutation_equivariant_mea  (None, 32, 32)           2080      \n",
      " n_2 (PermutationEquivariant                                     \n",
      " Mean)                                                           \n",
      "                                                                 \n",
      " qrelu_phi3 (QActivation)    (None, 32, 32)            0         \n",
      "                                                                 \n",
      " avgpool (GlobalAveragePooli  (None, 32)               0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " qdense_rho1 (QDense)        (None, 16)                528       \n",
      "                                                                 \n",
      " qrelu_rho1 (QActivation)    (None, 16)                0         \n",
      "                                                                 \n",
      " qdense_rho2 (QDense)        (None, 16)                272       \n",
      "                                                                 \n",
      " qrelu_rho2 (QActivation)    (None, 16)                0         \n",
      "                                                                 \n",
      " qdense_rho3 (QDense)        (None, 5)                 85        \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,281\n",
      "Trainable params: 5,275\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#########################################################################################################\n",
    "'''\n",
    "# Silence the info from tensorflow in which it brags that it can run on cpu nicely.\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "keras.utils.set_random_seed(123)\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "from util.terminal_colors import tcols\n",
    "from . import util as dsutil\n",
    "\n",
    "tf.keras.backend.set_floatx(\"float64\")\n",
    "\n",
    "util.util.device_info()\n",
    "outdir = util.util.make_output_directory(\"trained_deepsets\", args[\"outdir\"])\n",
    "util.util.save_hyperparameters_file(args, outdir)\n",
    "\n",
    "data = Data.shuffled(**args[\"data_hyperparams\"])\n",
    "'''\n",
    "#########################################################################################################\n",
    "\n",
    "# Quantized bits\n",
    "nbits=8\n",
    "integ=0\n",
    "\n",
    "#qbits = quantized_bits(nbits,integ,alpha=1.0)\n",
    "#qact = 'quantized_relu('+str(nbits)+',0)'\n",
    "\n",
    "# Set QKeras quantizer and activation \n",
    "if nbits == 1:\n",
    "    qbits = 'binary(alpha=1)'\n",
    "elif nbits == 2:\n",
    "    qbits = 'ternary(alpha=1)'\n",
    "else:\n",
    "    qbits = 'quantized_bits({},0,alpha=1)'.format(nbits)\n",
    "\n",
    "qact = 'quantized_relu({},0)'.format(nbits)\n",
    "\n",
    "# Print\n",
    "print(\"Training with max # of contituents = \", nconstit)\n",
    "print(\"Number of node features = \", nfeat)\n",
    "print(\"Quantization with nbits=\",nbits)\n",
    "print(\"Quantization of integer part=\",integ)\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "nnodes_phi = nconstit\n",
    "nnodes_rho = nconstit/2\n",
    "activ      = \"relu\"\n",
    "#activ      = \"elu\"\n",
    "\n",
    "# Number of target classes\n",
    "nclasses = len(Y_train_val[0]) \n",
    "\n",
    "# Instantiate Tensorflow input tensors in Batch mode \n",
    "inp = Input(shape=(nconstit,nfeat), name=\"inp\")   # Conv1D input format\n",
    "\n",
    "# Input point features BatchNormalization \n",
    "h = BatchNormalization(name='BatchNorm')(inp)\n",
    "\n",
    "# Phi MLP ( permutation equivariant layers )\n",
    "h = PermutationEquivariantMean(nnodes_phi, kernel_quantizer=qbits, bias_quantizer=qbits )(h)\n",
    "h = QActivation(qact,name='qrelu_phi1')(h)\n",
    "h = PermutationEquivariantMean(nnodes_phi, kernel_quantizer=qbits, bias_quantizer=qbits )(h)\n",
    "h = QActivation(qact,name='qrelu_phi2')(h)\n",
    "h = PermutationEquivariantMean(nnodes_phi, kernel_quantizer=qbits, bias_quantizer=qbits )(h)\n",
    "phi_out = QActivation(qact,name='qrelu_phi3')(h)\n",
    " \n",
    "# Agregate features (taking mean) over set elements  \n",
    "mean = GlobalAveragePooling1D(name='avgpool')(phi_out)      # return mean of features over elements\n",
    "#mean = tf.reduce_mean(phi_out, axis=1)                     # return mean of features over elements\n",
    " \n",
    "# Rho MLP\n",
    "h = QDense(nnodes_rho, kernel_quantizer=qbits, bias_quantizer=qbits,name='qdense_rho1' )(mean)\n",
    "h = QActivation(qact,name='qrelu_rho1')(h)\n",
    "h = QDense(nnodes_rho, kernel_quantizer=qbits, bias_quantizer=qbits,name='qdense_rho2' )(h)\n",
    "h = QActivation(qact,name='qrelu_rho2')(h)\n",
    "h = QDense(nclasses, kernel_quantizer=qbits, bias_quantizer=qbits,name='qdense_rho3' )(h)\n",
    "out = Activation('softmax',name='softmax')(h)\n",
    "\n",
    "# Build the model\n",
    "arch = 'QDeepSets_PermutationInv'\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "\n",
    "# Define the optimizer ( minimization algorithm )\n",
    "#optim = SGD(learning_rate=0.0001,decay=1e-6)\n",
    "#optim = Adam(learning_rate=0.0001)\n",
    "optim = Adam()\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# print the model summary\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlDvqPkIBn48"
   },
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9682,
     "status": "ok",
     "timestamp": 1616956534957,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "p3lHpPv-Bn49",
    "outputId": "252bb588-0b97-49ce-c444-60926c01c2f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 13:55:55.629158: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-05-06 13:55:56.521064: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807/807 [==============================] - ETA: 0s - loss: 1.1431 - categorical_accuracy: 0.5493"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 13:56:28.139872: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807/807 [==============================] - 42s 47ms/step - loss: 1.1431 - categorical_accuracy: 0.5493 - val_loss: 0.9445 - val_categorical_accuracy: 0.6467 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "807/807 [==============================] - 37s 46ms/step - loss: 0.9074 - categorical_accuracy: 0.6599 - val_loss: 0.8779 - val_categorical_accuracy: 0.6764 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "807/807 [==============================] - 37s 46ms/step - loss: 0.8664 - categorical_accuracy: 0.6771 - val_loss: 0.8543 - val_categorical_accuracy: 0.6867 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "807/807 [==============================] - 37s 46ms/step - loss: 0.8416 - categorical_accuracy: 0.6880 - val_loss: 0.8208 - val_categorical_accuracy: 0.6984 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "807/807 [==============================] - 37s 46ms/step - loss: 0.8324 - categorical_accuracy: 0.6911 - val_loss: 0.8316 - val_categorical_accuracy: 0.6893 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "807/807 [==============================] - 37s 46ms/step - loss: 0.8227 - categorical_accuracy: 0.6963 - val_loss: 0.8100 - val_categorical_accuracy: 0.7031 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "807/807 [==============================] - 37s 46ms/step - loss: 0.8171 - categorical_accuracy: 0.6995 - val_loss: 0.8032 - val_categorical_accuracy: 0.7060 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "807/807 [==============================] - 39s 48ms/step - loss: 0.8165 - categorical_accuracy: 0.6983 - val_loss: 0.7954 - val_categorical_accuracy: 0.7105 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "807/807 [==============================] - 37s 46ms/step - loss: 0.8101 - categorical_accuracy: 0.7019 - val_loss: 0.8047 - val_categorical_accuracy: 0.7046 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "807/807 [==============================] - 37s 46ms/step - loss: 0.8124 - categorical_accuracy: 0.7004 - val_loss: 0.8222 - val_categorical_accuracy: 0.6959 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "807/807 [==============================] - 38s 47ms/step - loss: 0.8173 - categorical_accuracy: 0.6970 - val_loss: 0.8357 - val_categorical_accuracy: 0.6855 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "807/807 [==============================] - 37s 46ms/step - loss: 0.8221 - categorical_accuracy: 0.6946 - val_loss: 0.8421 - val_categorical_accuracy: 0.6821 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "807/807 [==============================] - 37s 46ms/step - loss: 0.8313 - categorical_accuracy: 0.6899 - val_loss: 0.8227 - val_categorical_accuracy: 0.6960 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "807/807 [==============================] - 38s 47ms/step - loss: 0.8395 - categorical_accuracy: 0.6846 - val_loss: 0.8299 - val_categorical_accuracy: 0.6904 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "807/807 [==============================] - 36s 45ms/step - loss: 0.8468 - categorical_accuracy: 0.6801 - val_loss: 0.8558 - val_categorical_accuracy: 0.6738 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "807/807 [==============================] - 37s 45ms/step - loss: 0.8520 - categorical_accuracy: 0.6773 - val_loss: 0.8531 - val_categorical_accuracy: 0.6769 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "807/807 [==============================] - 37s 45ms/step - loss: 0.8596 - categorical_accuracy: 0.6720 - val_loss: 0.8546 - val_categorical_accuracy: 0.6754 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "807/807 [==============================] - 36s 45ms/step - loss: 0.8655 - categorical_accuracy: 0.6686 - val_loss: 0.8715 - val_categorical_accuracy: 0.6622 - lr: 0.0010\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m\n\u001b[1;32m     19\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit( X_train_val, Y_train_val, \n\u001b[1;32m     20\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, \n\u001b[1;32m     21\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, \n\u001b[1;32m     22\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     23\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[es,ls], \n\u001b[1;32m     24\u001b[0m                     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#                    callbacks=[early_stopping, model_checkpoint], \u001b[39;00m\n\u001b[1;32m     27\u001b[0m  \n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Set NN and output name\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m fname \u001b[38;5;241m=\u001b[39m arch\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_nconst_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[43mnmax\u001b[49m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_nbits_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(nbits)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaving Model : \u001b[39m\u001b[38;5;124m'\u001b[39m,fname)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m## Save the model+ weights\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nmax' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "# early stopping callback\n",
    "es = EarlyStopping(monitor='val_categorical_accuracy', patience=10)\n",
    "\n",
    "# Learning rate scheduler \n",
    "ls = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.2, patience=10)\n",
    "\n",
    "# model checkpoint callback\n",
    "# this saves our model architecture + parameters into mlp_model.h5\n",
    "chkp = ModelCheckpoint('QDeepSet_model.h5', monitor='val_categorical_accuracy', \n",
    "                                   verbose=0, save_best_only=True, \n",
    "                                   save_weights_only=False, mode='auto', \n",
    "                                   save_freq=1)\n",
    "\n",
    "#tb = TensorBoard(\"/Users/sznajder/WorkM1/miniforge3/tensorflow_macos/arm64/workdir/logs\")\n",
    "\n",
    "# Train classifier\n",
    "history = model.fit( X_train_val, Y_train_val, \n",
    "                    epochs=20, \n",
    "                    batch_size=512, \n",
    "                    verbose=1,\n",
    "                    callbacks=[es,ls], \n",
    "                    validation_split=0.3 )\n",
    "                    \n",
    "#                    callbacks=[early_stopping, model_checkpoint], \n",
    " \n",
    "#\n",
    "# Set NN and output name\n",
    "fname = arch+'_nconst_'+str(nconstit)+'_nbits_'+str(nbits)\n",
    "print('Saving Model : ',fname)\n",
    "\n",
    "\n",
    "## Save the model+ weights\n",
    "model.save('model_'+fname+'.h5')\n",
    "\n",
    "## Save the model weights in a separate file\n",
    "model.save_weights('weights_'+fname+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "np12nLtsBn5A"
   },
   "source": [
    "## Plot performance\n",
    "Here, we plot the history of the training and the performance in a ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "executionInfo": {
     "elapsed": 10466,
     "status": "ok",
     "timestamp": 1616956535743,
     "user": {
      "displayName": "Andre Sznajder",
      "photoUrl": "https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg",
      "userId": "12562331206892861623"
     },
     "user_tz": -120
    },
    "id": "Z_dscosMBn5B",
    "outputId": "6f9f7a67-1607-42ab-a9f8-5635b360eaec"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Plot loss vs epoch\n",
    "plt.figure(figsize=(15,10))\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax.plot(history.history['loss'], label='loss')\n",
    "ax.plot(history.history['val_loss'], label='val loss')\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "\n",
    "# Plot accuracy vs epoch\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "#ax.plot(history.history['accuracy'], label='accuracy')\n",
    "#ax.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "ax.plot(history.history['categorical_accuracy'], label='categorical_accuracy')\n",
    "ax.plot(history.history['val_categorical_accuracy'], label='val categorical accuracy')\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('acc')\n",
    "\n",
    "# Plot the ROC curves\n",
    "labels = ['gluon', 'quark', 'W', 'Z', 'top']\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "auc1 = {}\n",
    "precision = {}\n",
    "recall = {}\n",
    "NN = {}\n",
    "NP = {}\n",
    "TP = {}\n",
    "FP = {}\n",
    "TN = {}\n",
    "FN = {}\n",
    "tresholds = {}\n",
    "\n",
    "\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "Y_predict = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Loop over classes(labels) to get metrics per class\n",
    "for i, label in enumerate(labels):\n",
    "    fpr[label], tpr[label], tresholds[label] = roc_curve(Y_test[:,i], Y_predict[:,i])\n",
    "#    precision[label], recall[label], tresholds = precision_recall_curve(Y_test[:,i], Y_predict[:,i]) \n",
    "    print( np.unique(Y_test[:,i], return_counts=True) )\n",
    "    _ , N = np.unique(Y_test[:,i], return_counts=True) # count the NEGATIVES and POSITIVES samples in your test set\n",
    "    NN[label] = N[0]                   # number of NEGATIVES \n",
    "    NP[label] = N[1]                   # number of POSITIVES\n",
    "    TP[label] = tpr[label]*NP[label]\n",
    "    FP[label] = fpr[label]*NN[label] \n",
    "    TN[label] = NN[label] - FP[label]\n",
    "    FN[label] = NP[label] - TP[label]\n",
    "\n",
    "    auc1[label] = auc(fpr[label], tpr[label])\n",
    "    ax.plot(tpr[label],fpr[label],label='%s tagger, auc = %.1f%%'%(label,auc1[label]*100.))\n",
    "\n",
    "ax.semilogy()\n",
    "ax.set_xlabel(\"sig. efficiency\")\n",
    "ax.set_ylabel(\"bkg. mistag rate\")\n",
    "ax.set_ylim(0.001,1)\n",
    "#ax.set_grid(True)\n",
    "ax.legend(loc='lower right')\n",
    "#plt.savefig('%s/ROC.pdf'%(options.outputDir))\n",
    "\n",
    "\n",
    "\n",
    "# Plot DNN output \n",
    "ax = plt.subplot(2, 2, 4)\n",
    "X = np.linspace(0.0, 1.0, 20)\n",
    "hist={}\n",
    "for i, name in enumerate(labels):\n",
    "    hist[name] = ax.hist(Y_predict, bins=X, label=name ,histtype='step')\n",
    "ax.semilogy()\n",
    "ax.set_xlabel('DNN Output')\n",
    "ax.legend(prop={'size': 10})\n",
    "ax.legend(loc='lower left')\n",
    "\n",
    "\n",
    "# Display plots\n",
    "fig = plt.gcf()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Save plots\n",
    "fig.savefig(fname+'.pdf')\n",
    "\n",
    "\n",
    "# Save FPR for a given TPR value ( 30% , 50% & 80%)\n",
    "with open('FPR@TPR_'+fname+'.csv', 'w') as file:\n",
    "  file.write(\"model,label,treshold,tpr,fpr\\n\")\n",
    "  for label in labels:\n",
    "    for t in [0.3, 0.5, 0.8]:\n",
    "      index = np.argmax(tpr[label]>t)\n",
    "      file.write( arch+','+label+','+str(t)+','+str(tpr[label][index])+','+str(fpr[label][index])+'\\n' )\n",
    "      print(\"Label = \", label , \" with treshold = \",t)\n",
    "      print(\"TPR = \",tpr[label][index])\n",
    "      print(\"FPR = \",fpr[label][index])\n",
    "      print(\" \")\n",
    "               \n",
    "               \n",
    "# Save ROC AUC for each label\n",
    "with open('ROCAUC_'+fname+'.csv', 'w') as file:\n",
    "  header = labels[0]+', '+labels[1]+', '+labels[2]+', '+labels[3]+', '+labels[4]+'\\n'\n",
    "  file.write(header)\n",
    "  rocauc = str(auc1[labels[0]])+', '+str(auc1[labels[1]])+', '+str(auc1[labels[2]])+', '+str(auc1[labels[3]])+', '+str(auc1[labels[4]])\n",
    "  file.write(rocauc)\n",
    "\n",
    "\n",
    "\n",
    "# Save NN Accuracy for treshold of 0.5 for each label and the average over all classes\n",
    "acc_avg = float(accuracy_score (np.argmax(Y_test,axis=1), np.argmax(Y_predict,axis=1)))\n",
    "with open('ACCURACY_'+fname+'.csv', 'w') as file:\n",
    "  header = labels[0]+', '+labels[1]+', '+labels[2]+', '+labels[3]+', '+labels[4]+', '+'acc_avg'+'\\n'\n",
    "  file.write(header)\n",
    "  accuracy = ''\n",
    "  for label in labels:  \n",
    "    idx = np.argmax( tresholds[label] <= 0.5 )\n",
    "    accuracy += str( (TP[label][idx]+TN[label][idx])/(NP[label]+NN[label]) )+', '\n",
    "  accuracy += str(acc_avg) \n",
    "  file.write(accuracy)\n",
    "\n",
    "\n",
    "'''\n",
    "# Save confusion matrix ndarrays to .npz file\n",
    "with open('CONF_MATRIX_'+fname+'.npz', 'wb') as file:\n",
    "    vars = {}\n",
    "    vars[arch]=np.array(1) # save model name\n",
    "    for label in labels:\n",
    "        vars['tresholds_'+label+'_'+arch] = tresholds[label]\n",
    "        vars['TP_'+label+'_'+arch] = TP[label]\n",
    "        vars['FP_'+label+'_'+arch] = FP[label]\n",
    "        vars['TN_'+label+'_'+arch] = TN[label]\n",
    "        vars['FN_'+label+'_'+arch] = FN[label]\n",
    "        vars['TPR_'+arch] = tpr[label]\n",
    "        vars['FPR_'+arch] = fpr[label]\n",
    "        vars['NP_'+arch]= NP[label]\n",
    "        vars['NN_'+arch]= NN[label]\n",
    "        vars['auc_'+arch] = auc1[label] \n",
    "#        print(vars)\n",
    "    np.savez(file, **vars)\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Save a sample of events for HLS\n",
    "njets=3000\n",
    "print(X_test.shape)\n",
    "np.save('x_test.npy', X_test[0:njets,:])\n",
    "np.save('y_test.npy', Y_test[0:njets,:])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "JetTagging_MLP_TFKeras.ipynb",
   "provenance": [
    {
     "file_id": "1_LtW5Af1ruCzp6IH18NNj2K3Vbgqto-F",
     "timestamp": 1613800914899
    },
    {
     "file_id": "https://github.com/thongonary/machine_learning_vbscan/blob/master/5-conv2d.ipynb",
     "timestamp": 1551264063701
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
